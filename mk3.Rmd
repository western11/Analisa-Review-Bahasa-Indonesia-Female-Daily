---
title: "mk3"
author: "jojoecp"
date: "5/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background {.tabset}
## Objective
Projek ini bertujuan untuk:   
- Mencari topik yang dibicarakan user FemaleDaily.com pada setiap rating (topic modeling)   
- Bagaimana Sentiment yang diberikan user dapat memprediksi rating   
- Membangun model dengan akurasi terbaik dalam memprediksi rating produk

Data bersumber dari FemaleDaily.com dengan 3 produk yang berbeda. Data berisi review (teks) dan rating pengguna berbahasa Indonesia. Data akan diolah terpisah untuk menjawab pertanyaan penelitian/objective dari projek

Data gathering -> data cleaning -> Topic modeling -> association rule -> prediksi -> evaluasi

## Library
Berikut library yang digunakan. jika library tidak tersedia lakukan


```{r}
#install.packages("dplyr")
```

```{r warning=FALSE, message=FALSE}
library(dplyr)
library(textclean)
library(tm)
library(SnowballC)
library(stringr)
library(stringi)
library(arules)
library(plyr)
library(e1071)
library(parsnip)
library(yardstick)
library(caret)
library(rsample)
```


# Data Import
```{r}
# import data
dat <- read.csv("emina1.csv")
# rename kolom, subset, dan menghilangkan baris yang mempunyai missing value
dat <- dat %>% rename(
  "review" = "Ã¯..review"
) %>% select(review,rating) %>%
  na.omit()
# mengubah tipe kolom rating menjadi factor
dat$rating <- as.factor(dat$rating)
# cek jumlah review sesuai rating
table(dat$rating)
```

import stopwords bahasa Indonesia
```{r}
# add custom bahasa stopwords
bahasa.sw <- read.csv("Bahasa.stopwords.csv", header = F,fileEncoding = "UTF-8-BOM")
bahasa.sw <- as.character(bahasa.sw$V1)
bahasa.sw <- c(bahasa.sw, stopwords())
```

Import stemming dan slangword converter bahasa Indonesia
```{r}
# add custom stemming and slangword bahasa indonesia
stemm_indo <- read.csv("Stemming 2.csv",sep = ";")
oldstem <- as.character(stemm_indo$old)
newstem <- as.character(stemm_indo$new)

slang_indo <- read.csv("Slangword2.csv",sep = ";")
oldslang <- as.character(slang_indo$old)
newslang <- as.character(slang_indo$new)
```

contoh review
```{r}
head(dat$review)
```

# Text Cleaning
Teks perlu dibersihkan sebelum dilakukan topic modeling dan prediksi. Tahap pembersihan yang dilakukan adalah:    
- Mengubah seluruh teks menjadi huruf kecil (lower)   
- Mengubah contraction (contoh: i've -> i have, you're -> you are)   
- mengubah slang words (contoh: gw -> aku, bgt -> banget, bagussss -> bagus)   
- menghapus emoji, emoticon, hash, angka, date, time, dan punctuation (/ , . " ;)   
- menghapus spasi berlebih antar kata (whitespace)   
- menghapus imbuhan (contoh: membeli -> beli, harganya -> harga)   
Setelah dibersihkan, data lalu dirubah ke bentuk corpus dan melakukan tokenization (perhitungan jumlah per kata dalam tiap review). Proses pembersihan sampai pembentukan corpus dibuat kedalam function `textcleaner` agar function bisa digunakan untuk tahapan selanjutnya. 

## membuat `textcleaner` function
```{r}
# membuat function untuk stemming bahasa Indonesia
stemmword <- function(x) Reduce(function(x,r) gsub(stemm_indo$old[r],stemm_indo$new[r],x,fixed=T),
seq_len(nrow(stemm_indo)),x)

# membuat function untuk konversi slangword bahasa Indonesia
slangword <- function(x) Reduce(function(x,r) gsub(slang_indo$old[r],slang_indo$new[r],x,fixed=T),
seq_len(nrow(slang_indo)),x)

# membuat function textcleaner
textcleaner <- function(x){
  x <- as.character(x)
  
  x <- x %>%
    str_to_lower() %>%  # convert all the string to low alphabet
    replace_contraction() %>% # replace contraction to their multi-word forms
    replace_internet_slang() %>% # replace internet slang to normal words
    replace_emoji() %>% # replace emoji to words
    replace_emoticon() %>% # replace emoticon to words
    replace_hash(replacement = "") %>% # remove hashtag
    replace_word_elongation() %>% # replace informal writing with known semantic replacements
    replace_number(remove = T) %>% # remove number
    replace_date(replacement = "") %>% # remove date
    replace_time(replacement = "") %>% # remove time
    str_remove_all(pattern = "[[:punct:]]") %>% # remove punctuation
    str_remove_all(pattern = "[^\\s]*[0-9][^\\s]*") %>% # remove mixed string n number
    str_squish() %>% # reduces repeated whitespace inside a string.
    str_trim() # removes whitespace from start and end of string
  
  xdtm <- VCorpus(VectorSource(x)) %>%
    tm_map(removeWords, stopwords("en")) %>%
    tm_map(removeWords, bahasa.sw) %>%
    tm_map(removePunctuation) %>%
    tm_map(content_transformer(stemmword)) %>%
    tm_map(content_transformer(slangword))
  
  # mengubah corpus menjadi document term matrix
  return(DocumentTermMatrix(xdtm))
    
}

```



# Topic Modeling
Topic modeling dilakukan untuk menjawab pertanyaan penelitian nomor 1, yaitu **Apa topik yang dibicarakan user FemaleDaily.com pada setiap rating?** Algoritma yang digunakan untuk mencari topic modeling adalah *Latent Dirichlet Allocation (LDA)*. LDA adalah model matematika yang digunakan untuk menemukan campuran kata yang terkait dengan setiap topik, juga menentukan campuran topik yang menggambarkan setiap dokumen. LDA bekerja untuk menjawab 2 prinsip topic modeling berikut:   

- Every document is a mixture of topics   
- Every topic is a mixture of words   


load library yang dibutuhkan
```{r warning=FALSE, message=FALSE}
library(topicmodels)
library(tidytext)
library(ggplot2)
```

Memisahkan review berdasarkan rating
```{r}
dat_5 <- dat %>% filter(rating == 5)
dat_4 <- dat %>% filter(rating == 4)
dat_3 <- dat %>% filter(rating == 3)
dat_2 <- dat %>% filter(rating == 2)
dat_1 <- dat %>% filter(rating == 1)
```

## Topic modeling rating 5
Sebelum melakukan topic modeling, perlu dilakukan pembersihan teks sama seperti modeling prediksi di tahap sebelumnya. Di modeling prediksi pembersihan dilakukan seara menyeluruh tanpa melihat rating. Disini dilakukan pembersihan ulang dengan data yang sudah di subset. Pengerjan topic modeling akan dibuat 5x sesuai banyak rating

Proses pembersihan teks
```{r}
# mengaplikasikan function textcleaner pada review dengan rating 5
dat_5_dtm <- textcleaner(dat_5$review)
# Filter kata yang ada di lebih dari 5 review
freqterm_5 <- findFreqTerms(dat_5_dtm, 10)
dat_5_dtm <- dat_5_dtm[,freqterm_5]
# setelah filtering akan ada data sisa tanpa isi token. data sperti itu akan dihilangkan
row_num5 <- apply(dat_5_dtm,1,sum)
dat_5_dtm <- dat_5_dtm[row_num5>0,]
```

mengaplikasikan algoritma LDA untuk mencari topic modeling. Disini akan dibuat 5 topik pembicaraan pada setiap rating
```{r}
lda_5 <- LDA(dat_5_dtm, k = 2, control = list(seed=1502))
topic_5 <- tidy(lda_5,matrix="beta")
```

```{r}
word_5 <- topic_5 %>% group_by(term) %>%
  summarise(mean_beta = mean(beta)) %>%
  arrange(-mean_beta) %>% select("term") %>% slice(1:250)

word_5
```

Membuat visualisasi topik yang dibicarakan
```{r}
top_terms_5 <- topic_5 %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_5 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Topik pembicaraan pada review di rating 5 FemaleDaily",
       subtitle = "Produk sunscreen Emina")
```


## Topic modeling rating 4

Proses pembersihan teks
```{r}
# mengaplikasikan function textcleaner pada review dengan rating 4
dat_4_dtm <- textcleaner(dat_4$review)
# Filter kata yang ada di lebih dari 10 review
freqterm_4 <- findFreqTerms(dat_4_dtm, 10)
dat_4_dtm <- dat_4_dtm[,freqterm_4]
# setelah filtering akan ada data sisa tanpa isi token. data sperti itu akan dihilangkan
row_num4 <- apply(dat_4_dtm,1,sum)
dat_4_dtm <- dat_4_dtm[row_num4>0,]
```

mengaplikasikan algoritma LDA untuk mencari topic modeling. Disini akan dibuat 5 topik pembicaraan pada setiap rating
```{r}
lda_4 <- LDA(dat_4_dtm, k = 2, control = list(seed=1502))
topic_4 <- tidy(lda_4,matrix="beta")
```

```{r}
word_4 <- topic_4 %>% group_by(term) %>%
  summarise(mean_beta = mean(beta)) %>%
  arrange(-mean_beta) %>% select("term") %>% slice(1:250)

word_4
```


## Topic modeling rating 3
Proses pembersihan teks
```{r}
# mengaplikasikan function textcleaner pada review dengan rating 3
dat_3_dtm <- textcleaner(dat_3$review)
# Filter kata yang ada di lebih dari 8 review
freqterm_3 <- findFreqTerms(dat_3_dtm, 8)
dat_3_dtm <- dat_3_dtm[,freqterm_3]
# setelah filtering akan ada data sisa tanpa isi token. data sperti itu akan dihilangkan
row_num3 <- apply(dat_3_dtm,1,sum)
dat_3_dtm <- dat_3_dtm[row_num3>0,]
```

mengaplikasikan algoritma LDA untuk mencari topic modeling. Disini akan dibuat 5 topik pembicaraan pada setiap rating
```{r}
lda_3 <- LDA(dat_3_dtm, k = 2, control = list(seed=1502))
topic_3 <- tidy(lda_3,matrix="beta")
```

```{r}
word_3 <- topic_3 %>% group_by(term) %>%
  summarise(mean_beta = mean(beta)) %>%
  arrange(-mean_beta) %>% select("term") %>% slice(1:250)

word_3
```

## Topic modeling rating 2
Proses pembersihan teks
```{r}
# mengaplikasikan function textcleaner pada review dengan rating 2
dat_2_dtm <- textcleaner(dat_2$review)
# Filter kata yang ada di lebih dari 5 review
freqterm_2 <- findFreqTerms(dat_2_dtm, 5)
dat_2_dtm <- dat_2_dtm[,freqterm_2]
# setelah filtering akan ada data sisa tanpa isi token. data sperti itu akan dihilangkan
row_num2 <- apply(dat_2_dtm,1,sum)
dat_2_dtm <- dat_2_dtm[row_num2>0,]
```

mengaplikasikan algoritma LDA untuk mencari topic modeling. Disini akan dibuat 5 topik pembicaraan pada setiap rating
```{r}
lda_2 <- LDA(dat_2_dtm, k = 2, control = list(seed=1502))
topic_2 <- tidy(lda_2,matrix="beta")
```

```{r}
word_2 <- topic_2 %>% group_by(term) %>%
  summarise(mean_beta = mean(beta)) %>%
  arrange(-mean_beta) %>% select("term") %>% slice(1:250)

word_2
```

## Topic modeling rating 1

Proses pembersihan teks
```{r}
# mengaplikasikan function textcleaner pada review dengan rating 1
dat_1_dtm <- textcleaner(dat_1$review)
# Filter kata yang ada di lebih dari 1 review
freqterm_1 <- findFreqTerms(dat_1_dtm, 3)
dat_1_dtm <- dat_1_dtm[,freqterm_1]
# setelah filtering akan ada data sisa tanpa isi token. data sperti itu akan dihilangkan
row_num1 <- apply(dat_1_dtm,1,sum)
dat_1_dtm <- dat_1_dtm[row_num1>0,]
```

mengaplikasikan algoritma LDA untuk mencari topic modeling. Disini akan dibuat 5 topik pembicaraan pada setiap rating
```{r}
lda_1 <- LDA(dat_1_dtm, k = 5, control = list(seed=1502))
topic_1 <- tidy(lda_1,matrix="beta")
```

```{r}
word_1 <- topic_1 %>% group_by(term) %>%
  summarise(mean_beta = mean(beta)) %>%
  arrange(-mean_beta) %>% select("term") %>% slice(1:250)

word_1
```

Membuat visualisasi topik yang dibicarakan
```{r}
top_terms_1 <- topic_1 %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_1 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Topik pembicaraan pada review di rating 1 FemaleDaily",
       subtitle = "Produk sunscreen Emina")
```


## membuat password
menggabungkan semua highest beta average word dari ke-5 rating
```{r}
password <- rbind(word_1,word_2,word_3,word_4,word_5)
# remove duplicate rows
password <- password[!duplicated(password),]
```

# Text Association Rules based on topic modeling

text cleaner tanpa dtm dan stopword
```{r}
# membuat function textcleaner
textcleaner_asc <- function(x){
  x <- as.character(x)
  
  x <- x %>%
    str_to_lower() %>%  # convert all the string to low alphabet
    replace_contraction() %>% # replace contraction to their multi-word forms
    replace_internet_slang() %>% # replace internet slang to normal words
    replace_emoji() %>% # replace emoji to words
    replace_emoticon() %>% # replace emoticon to words
    replace_hash(replacement = "") %>% # remove hashtag
    replace_word_elongation() %>% # replace informal writing with known semantic replacements
    replace_number(remove = T) %>% # remove number
    replace_date(replacement = "") %>% # remove date
    replace_time(replacement = "") %>% # remove time
    str_remove_all(pattern = "[[:punct:]]") %>% # remove punctuation
    str_remove_all(pattern = "[^\\s]*[0-9][^\\s]*") %>% # remove mixed string n number
    str_squish() %>% # reduces repeated whitespace inside a string.
    str_trim() # removes whitespace from start and end of string
  
  return(as.data.frame(x))
    
}

```

## word rules rating 5

```{r}
# clean text
asc_5 <- textcleaner_asc(dat_5$review)
# simpan word dari topic modeling yang akan digunakan untuk membuat rules
keepword_5 <- as.character(unique(topic_5$term))

# filter teks tiap review hanya mengambil word yang ada di keepword
clean_5 <- data.frame()
for(i in seq_along(asc_5$x)){
  clean <- paste(intersect(strsplit(asc_5$x,"\\s")[[i]],keepword_5),collapse = " ")
  bb <- data.frame(clean)
  
  clean_5 <- rbind(clean_5,bb)
}

clean_5
```

```{r}
# mengumah tiap word yang sudah bersih menjadi kolom
split_clean5 <- stri_split_fixed(clean_5$clean," ",simplify = T)
split_clean5 <- as.data.frame(split_clean5)
# simpan teks dengan kolom
write.csv(split_clean5,"split_clean5.csv",row.names = F)
```

```{r}
# read data teks sebagai data transaction untuk dibuatkan model association rules
trans_5 <- read.transactions("split_clean5.csv",sep = ",",header = T)
# modeling association rules menggunakan algoritma apriori
rules_5 <- apriori(trans_5,parameter = list(supp = 0.1, conf = 0.5))
# ubah hasil association rules menjadi dataframe
word_rules_5 <- data.frame(inspect(rules_5))
# simpan hasil association rules
write.csv(word_rules_5,"rules5.csv",row.names = F)
```

## word rules rating 4

```{r}
# clean text
asc_4 <- textcleaner_asc(dat_4$review)
# simpan word dari topic modeling yang akan digunakan untuk membuat rules
keepword_4 <- as.character(unique(topic_4$term))

# filter teks tiap review hanya mengambil word yang ada di keepword
clean_4 <- data.frame()
for(i in seq_along(asc_4$x)){
  clean <- paste(intersect(strsplit(asc_4$x,"\\s")[[i]],keepword_4),collapse = " ")
  bb <- data.frame(clean)
  
  clean_4 <- rbind(clean_4,bb)
}

clean_4
```

```{r}
# mengumah tiap word yang sudah bersih menjadi kolom
split_clean4 <- stri_split_fixed(clean_4$clean," ",simplify = T)
split_clean4 <- as.data.frame(split_clean4)
# simpan teks dengan kolom
write.csv(split_clean4,"split_clean4.csv",row.names = F)
```

```{r}
# read data teks sebagai data transaction untuk dibuatkan model association rules
trans_4 <- read.transactions("split_clean4.csv",sep = ",",header = T)
# modeling association rules menggunakan algoritma apriori
rules_4 <- apriori(trans_4,parameter = list(supp = 0.1, conf = 0.5))
# ubah hasil association rules menjadi dataframe
word_rules_4 <- data.frame(inspect(rules_4))
# simpan hasil association rules
write.csv(word_rules_4,"rules4.csv",row.names = F)
```

## word rules rating 3

```{r}
# clean text
asc_3 <- textcleaner_asc(dat_3$review)
# simpan word dari topic modeling yang akan digunakan untuk membuat rules
keepword_3 <- as.character(unique(topic_3$term))

# filter teks tiap review hanya mengambil word yang ada di keepword
clean_3 <- data.frame()
for(i in seq_along(asc_3$x)){
  clean <- paste(intersect(strsplit(asc_3$x,"\\s")[[i]],keepword_3),collapse = " ")
  bb <- data.frame(clean)
  
  clean_3 <- rbind(clean_3,bb)
}

clean_3
```

```{r}
# mengumah tiap word yang sudah bersih menjadi kolom
split_clean3 <- stri_split_fixed(clean_3$clean," ",simplify = T)
split_clean3 <- as.data.frame(split_clean3)
# simpan teks dengan kolom
write.csv(split_clean3,"split_clean3.csv",row.names = F)
```

```{r}
# read data teks sebagai data transaction untuk dibuatkan model association rules
trans_3 <- read.transactions("split_clean3.csv",sep = ",",header = T)
# modeling association rules menggunakan algoritma apriori
rules_3 <- apriori(trans_3,parameter = list(supp = 0.1, conf = 0.5))
# ubah hasil association rules menjadi dataframe
word_rules_3 <- data.frame(inspect(rules_3))
# simpan hasil association rules
write.csv(word_rules_3,"rules3.csv",row.names = F)
```

## word rules rating 2

```{r}
# clean text
asc_2 <- textcleaner_asc(dat_2$review)
# simpan word dari topic modeling yang akan digunakan untuk membuat rules
keepword_2 <- as.character(unique(topic_2$term))

# filter teks tiap review hanya mengambil word yang ada di keepword
clean_2 <- data.frame()
for(i in seq_along(asc_2$x)){
  clean <- paste(intersect(strsplit(asc_2$x,"\\s")[[i]],keepword_2),collapse = " ")
  bb <- data.frame(clean)
  
  clean_2 <- rbind(clean_2,bb)
}

clean_2
```

```{r}
# mengumah tiap word yang sudah bersih menjadi kolom
split_clean2 <- stri_split_fixed(clean_2$clean," ",simplify = T)
split_clean2 <- as.data.frame(split_clean2)
# simpan teks dengan kolom
write.csv(split_clean2,"split_clean2.csv",row.names = F)
```

```{r}
# read data teks sebagai data transaction untuk dibuatkan model association rules
trans_2 <- read.transactions("split_clean2.csv",sep = ",",header = T)
# modeling association rules menggunakan algoritma apriori
rules_2 <- apriori(trans_2,parameter = list(supp = 0.1, conf = 0.5))
# ubah hasil association rules menjadi dataframe
word_rules_2 <- data.frame(inspect(rules_2))
# simpan hasil association rules
write.csv(word_rules_2,"rules2.csv",row.names = F)
```

## word rules rating 1

```{r}
# clean text
asc_1 <- textcleaner_asc(dat_1$review)
# simpan word dari topic modeling yang akan digunakan untuk membuat rules
keepword_1 <- as.character(unique(topic_1$term))

# filter teks tiap review hanya mengambil word yang ada di keepword
clean_1 <- data.frame()
for(i in seq_along(asc_1$x)){
  clean <- paste(intersect(strsplit(asc_1$x,"\\s")[[i]],keepword_1),collapse = " ")
  bb <- data.frame(clean)
  
  clean_1 <- rbind(clean_1,bb)
}

clean_1
```

```{r}
# mengumah tiap word yang sudah bersih menjadi kolom
split_clean1 <- stri_split_fixed(clean_1$clean," ",simplify = T)
split_clean1 <- as.data.frame(split_clean1)
# simpan teks dengan kolom
write.csv(split_clean1,"split_clean1.csv",row.names = F)
```

```{r}
# read data teks sebagai data transaction untuk dibuatkan model association rules
trans_1 <- read.transactions("split_clean1.csv",sep = ",",header = T)
# modeling association rules menggunakan algoritma apriori
rules_1 <- apriori(trans_1,parameter = list(supp = 0.1, conf = 0.5))
# ubah hasil association rules menjadi dataframe
word_rules_1 <- data.frame(inspect(rules_1))
# simpan hasil association rules
write.csv(word_rules_1,"rules1.csv",row.names = F)
```


# sentiment analysis
```{r}
sentimen_indo <- read.csv("password_sent.csv")

# set word sentimen positif
sentimen_pos <- sentimen_indo %>% filter(sentiment == "positive")
sentimen_pos <- sentimen_pos$term %>% str_trim() %>% str_squish() %>%
  as.character()

# set word sentimen negatif
sentimen_neg <- sentimen_indo %>% filter(sentiment == "negative")
sentimen_neg <- sentimen_neg$term %>% str_trim() %>% str_squish() %>%
  as.character()
```

```{r}
score.sentiment = function(kalimat2, sentimen_pos, sentimen_neg, .progress='none')
{
  require(plyr)
  require(stringr)
  scores = laply(kalimat2, function(kalimat, sentimen_pos, sentimen_neg) {
    kalimat = gsub('[[:punct:]]', '', kalimat)
    kalimat = gsub('[[:cntrl:]]', '', kalimat)
    kalimat = gsub('\\d+', '', kalimat)
    kalimat = tolower(kalimat)
    
    list.kata = str_split(kalimat, '\\s+')
    kata2 = unlist(list.kata)
    positif.matches = match(kata2, sentimen_pos)
    negatif.matches = match(kata2, sentimen_neg)
    positif.matches = !is.na(positif.matches)
    negatif.matches = !is.na(negatif.matches)
    score = sum(positif.matches) - (sum(negatif.matches))
    return(score)
  }, sentimen_pos, sentimen_neg, .progress=.progress )
  scores.df = data.frame(score=scores, text=kalimat2)
  return(scores.df)
}
```

```{r}
library(scales)
```

## Sentiment rating 5
```{r}
sentimen_5 <- score.sentiment(dat_5$review,sentimen_pos,sentimen_neg)
sentimen_5$sentiment <- ifelse(sentimen_5$score <=0,"Negatif","Positif") %>% as.factor()

data.frame(table(sentimen_5$sentiment)) %>%
  ggplot(aes(x = Var1, y = Freq)) +
  geom_col(aes(fill = Var1)) + theme_minimal() +
  geom_label(aes(label = scales::percent(Freq/length(sentimen_5$text))),size = 4) +
  geom_text(aes(label = Freq), nudge_y = 130, show.legend = F) +
  labs(title = "Sentiment Produk Emina",
       subtitle = "Rating review 5",
       x = "Sentiment", y = "Frequency",
       fill = "") +
  theme(legend.position = "bottom")
```

## Sentiment rating 4
```{r}
sentimen_4 <- score.sentiment(dat_4$review,sentimen_pos,sentimen_neg)
sentimen_4$sentiment <- ifelse(sentimen_4$score <=0,"Negatif","Positif") %>% as.factor()

data.frame(table(sentimen_4$sentiment)) %>%
  ggplot(aes(x = Var1, y = Freq)) +
  geom_col(aes(fill = Var1)) + theme_minimal() +
  geom_label(aes(label = scales::percent(Freq/length(sentimen_4$text))),size = 4) +
  geom_text(aes(label = Freq), nudge_y = 140, show.legend = F) +
  labs(title = "Sentiment Produk Emina",
       subtitle = "Rating review 4",
       x = "Sentiment", y = "Frequency",
       fill = "") +
  theme(legend.position = "bottom")
```

## Sentiment rating 3
```{r}
sentimen_3 <- score.sentiment(dat_3$review,sentimen_pos,sentimen_neg)
sentimen_3$sentiment <- ifelse(sentimen_3$score <=0,"Negatif","Positif") %>% as.factor()

data.frame(table(sentimen_3$sentiment)) %>%
  ggplot(aes(x = Var1, y = Freq)) +
  geom_col(aes(fill = Var1)) + theme_minimal() +
  geom_label(aes(label = scales::percent(Freq/length(sentimen_3$text))),size = 4) +
  geom_text(aes(label = Freq), nudge_y = 100, show.legend = F) +
  labs(title = "Sentiment Produk Emina",
       subtitle = "Rating review 3",
       x = "Sentiment", y = "Frequency",
       fill = "") +
  theme(legend.position = "bottom")
```
## Sentiment rating 2
```{r}
sentimen_2 <- score.sentiment(dat_2$review,sentimen_pos,sentimen_neg)
sentimen_2$sentiment <- ifelse(sentimen_2$score <=0,"Negatif","Positif") %>% as.factor()

data.frame(table(sentimen_2$sentiment)) %>%
  ggplot(aes(x = Var1, y = Freq)) +
  geom_col(aes(fill = Var1)) + theme_minimal() +
  geom_label(aes(label = scales::percent(Freq/length(sentimen_2$text))),size = 4) +
  geom_text(aes(label = Freq), nudge_y = 45, show.legend = F) +
  labs(title = "Sentiment Produk Emina",
       subtitle = "Rating review 2",
       x = "Sentiment", y = "Frequency",
       fill = "") +
  theme(legend.position = "bottom")
```
## Sentiment rating 1
```{r}
sentimen_1 <- score.sentiment(dat_1$review,sentimen_pos,sentimen_neg)
sentimen_1$sentiment <- ifelse(sentimen_1$score <=0,"Negatif","Positif") %>% as.factor()

data.frame(table(sentimen_1$sentiment)) %>%
  ggplot(aes(x = Var1, y = Freq)) +
  geom_col(aes(fill = Var1)) + theme_minimal() +
  geom_label(aes(label = scales::percent(Freq/length(sentimen_1$text))),size = 4) +
  geom_text(aes(label = Freq), nudge_y = 18, show.legend = F) +
  labs(title = "Sentiment Produk Emina",
       subtitle = "Rating review 1",
       x = "Sentiment", y = "Frequency",
       fill = "") +
  theme(legend.position = "bottom")
```

# Prediksi rating berdasarkan review


```{r}
textcleaner_clf <- function(x){
  x <- as.character(x)
  
  x <- x %>%
    str_to_lower() %>%  # convert all the string to low alphabet
    replace_contraction() %>% # replace contraction to their multi-word forms
    replace_internet_slang() %>% # replace internet slang to normal words
    replace_emoji() %>% # replace emoji to words
    replace_emoticon() %>% # replace emoticon to words
    replace_hash(replacement = "") %>% # remove hashtag
    replace_word_elongation() %>% # replace informal writing with known semantic replacements
    replace_number(remove = T) %>% # remove number
    replace_date(replacement = "") %>% # remove date
    replace_time(replacement = "") %>% # remove time
    str_remove_all(pattern = "[[:punct:]]") %>% # remove punctuation
    str_remove_all(pattern = "[^\\s]*[0-9][^\\s]*") %>% # remove mixed string n number
    str_squish() %>% # reduces repeated whitespace inside a string.
    str_trim() # removes whitespace from start and end of string
  
  xdtm <- VCorpus(VectorSource(x)) %>%
    tm_map(removeWords, stopwords("en")) %>%
    tm_map(removeWords, bahasa.sw) %>%
    tm_map(removePunctuation) %>%
    tm_map(content_transformer(stemmword)) %>%
    tm_map(content_transformer(slangword))
  
  # mengubah corpus menjadi document term matrix
  return(DocumentTermMatrix(xdtm,control = list(
    dictionary = password$term
  )))
}
```

## proses cleaning
```{r}
# mengaplikasikan function textclean ke review produk
review.dtm <- textcleaner_clf(dat$review)
```

```{r}
# mengubah term matrix menjadi dataframe untuk modeling
dat.clean <- as.data.frame(as.matrix(review.dtm), stringsAsFactors = F)
# we have 800+ variable in words form. i change the label name from `mood` to labelY to avoid overwriting column names
new.dat <- cbind(dat.clean, data.frame(labelY = dat$rating))
# hasil data bersih yang sudah diberi token
# tiap baris menunjukkan 1 review user
head(new.dat)
```

## Modeling
Untuk menjawab pertanyaan penelitian 3, dilakukan modeling dengan algoritma Naive Bayes dan Random Forest (more model soon). Akurasi model didapat dari prediksi test data (unseen data). **Note:** metode yang dilakukan ada klasifikasi, jadi rating dianggap tidak mempunyai level. rating 1 akan dianggap sama dengan rating 5 and vice versa

### splitting
split data menjadi data train dan test dengan proporsi 75% dan 25%. data train digunakan untuk membangun model dan test untuk evaluasi model (prediksi)
```{r}
set.seed(1502) # making sample reproduciable
splitter <- initial_split(new.dat, prop = 0.75, strata = "labelY")
train <- training(splitter)
test <- testing(splitter)
```



### Naive Bayes
Naive bayes membutuhkan format data yang berbeda. Naive bayes tidak membutuhkan frekuensi token. token hanya berisi 1 dan 0, 1 menunjukkan kata tersebut ada didalam kalimat dan 0 brarti tidak ada kehadiran kata tersebut
```{r}
# split the data. 75% for train data, and 25% for test data
set.seed(1502)
index <- sample(1:nrow(dat_dtm), 0.75*nrow(dat_dtm))

train_x <- dat_dtm[index,]
test_x <- dat_dtm[-index,]
# subset label/target variable
train_label <- dat[index,"rating"]
test_label <- dat[-index,"rating"]
```

Use bernoulli converter to convert any value above 0 to 1 and 0 to remain 0.
```{r}
# bernoulli conv 
bernoulli_conv <- function(x){
  x <- as.factor(as.numeric(x>0))
}
```

apply bernoulli converter to train and test data
```{r}
train_x <- apply(train_x,2,bernoulli_conv)
test_x <- apply(test_x,2,bernoulli_conv)
```

Membangun model naive bayes
```{r}
# train the model
mod.nb <- naiveBayes(train_x, as.factor(train_label), laplace = 1)

# predict to test data
pred.nb <- predict(mod.nb, test_x,
                   type = "class")

pred.nb.x <- cbind(data.frame(pred.nb),as.factor(test_label))%>%
  setNames(c("pred","actual"))

pred.nb.x
```

Membuat confusion matrix untuk evaluasi hasil prediksi model
```{r}
cf.nb <- confusionMatrix(data = pred.nb.x$pred,
                         reference = pred.nb.x$actual)
cf.nb
```
### Random Forest
model random forest dengan mesin `ranger` tidak dapat menerima nama kolom dengan special character seperti for, break, next, return, dll. Perlu dilakukan koversi nama kolom menjadi nama yang bukan special character
```{r}
# this chunks are made for random forest model and future model tuning
## the column names like break,for,next,if are considered as special character thus raises an error when building random forest and model tuning.
## i store the train and test data to new variabel so the old one remain reproducible
train_tune <- train
test_tune <- test

colnames(train_tune) <- make.names(colnames(train_tune))
colnames(test_tune) <- make.names(colnames(test_tune))

# build 5 folds cross validation for tuning evaluation
set.seed(1502)
folds <- vfold_cv(train_tune, 5)
```

Membangun model Random Forest
```{r}
mod.rf <- rand_forest(trees = 550, mtry = 6, mode = "classification") %>%
  set_engine("ranger") %>% fit(labelY~., data = train_tune)

pred.rf <- predict(mod.rf, test_tune, 
                   type = "class")

pred.rf.x <- as.data.frame(cbind(pred.rf, test_tune$labelY)) %>%
  setNames(c("pred","actual"))

pred.rf.x
```

Membuat confusion matrix untuk evaluasi hasil prediksi model
```{r}
cf.rf <- confusionMatrix(data = pred.rf.x$pred,
                         reference = pred.rf.x$actual)
cf.rf
```





